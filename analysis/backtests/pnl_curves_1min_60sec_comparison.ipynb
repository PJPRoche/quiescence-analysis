{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# P&L Curves Comparison\n",
    "\n",
    "This notebook visualizes and compares cumulative P&L curves across multiple backtest runs.\n",
    "\n",
    "## Features\n",
    "\n",
    "- Overlay multiple P&L curves on a single plot for easy comparison\n",
    "- Interactive plotly charts with hover tooltips and legend controls\n",
    "- Fast metadata-based filtering before loading full data\n",
    "- Color-coded traces for distinguishing different runs\n",
    "\n",
    "## Usage\n",
    "\n",
    "1. Configure stock symbol and paths\n",
    "2. Scan available runs to see metadata\n",
    "3. Filter runs using the summary DataFrame\n",
    "4. Load selected runs\n",
    "5. Visualize P&L curves\n",
    "\n",
    "Use this notebook to quickly identify which parameter combinations lead to better performance profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add analysis folder to path for imports\n",
    "# Use the project root to construct the analysis path reliably\n",
    "project_root = Path.cwd()\n",
    "while project_root.name != 'quiescence' and project_root.parent != project_root:\n",
    "    project_root = project_root.parent\n",
    "analysis_path = project_root / 'analysis'\n",
    "\n",
    "if str(analysis_path) not in sys.path:\n",
    "    sys.path.insert(0, str(analysis_path))\n",
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import custom utility functions from analysis folder\n",
    "from utilities import (\n",
    "    scan_backtest_runs, \n",
    "    load_run_data, \n",
    "    create_runs_summary_dataframe, \n",
    "    convert_utc_to_ny,\n",
    "    build_cumulative_pnl_from_positions\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Set the stock symbol and paths for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "stock_symbol = \"MSFT\"\n",
    "\n",
    "# Project paths\n",
    "STORAGE_ROOT = Path(\"/data/quiescence/\")\n",
    "BACKTEST_ROOT = STORAGE_ROOT / \"backtest\"\n",
    "\n",
    "print(f\"Analyzing stock: {stock_symbol}\")\n",
    "print(f\"Backtest runs root: {BACKTEST_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2. Scan and Filter Runs\n",
    "\n",
    "First, scan all available runs and create a summary DataFrame for filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scan all runs for the configured stock (fast - metadata only)\n",
    "runs_metadata = scan_backtest_runs(BACKTEST_ROOT, stock_symbol)\n",
    "\n",
    "# Create summary DataFrame for easy filtering and comparison\n",
    "df_summary = create_runs_summary_dataframe(runs_metadata)\n",
    "\n",
    "print(f\"\\nFound {len(df_summary)} runs for {stock_symbol}\\n\")\n",
    "print(df_summary.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3. Select Runs to Load\n",
    "\n",
    "Choose which runs to analyze. You can load all, specific runs by number, or filter by parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load ALL runs\n",
    "# runs_data = [load_run_data(run) for run in runs_metadata]\n",
    "\n",
    "# Option 2: Load specific runs by Run number\n",
    "selected_run_numbers = [11, 19]\n",
    "runs_data = [load_run_data(runs_metadata[run_num - 1]) for run_num in selected_run_numbers if 0 < run_num <= len(runs_metadata)]\n",
    "\n",
    "# Option 3: Filter using DataFrame conditions\n",
    "# Example: Load runs with specific parameter values\n",
    "# filtered_df = df_summary[\n",
    "#     (df_summary['Entry P Top'].astype(float) >= 0.95) & \n",
    "#     (df_summary['Frequency'] == '1-MINUTE')\n",
    "# ]\n",
    "# selected_indices = filtered_df['Run'].values - 1\n",
    "# runs_data = [load_run_data(runs_metadata[i]) for i in selected_indices]\n",
    "\n",
    "print(f\"Loaded {len(runs_data)} runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 5. Visualize P&L Curves\n",
    "\n",
    "Create an interactive plot overlaying all selected runs' P&L curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add each run's P&L curve to the same plot\n",
    "for i, run in enumerate(runs_data):\n",
    "    # Extract frequency from bar_type\n",
    "    bar_type = run.get('bar_type', '')\n",
    "    if bar_type:\n",
    "        parts = bar_type.split('-')\n",
    "        frequency = f\"{parts[1]}{parts[2].lower()[0:3]}\" if len(parts) >= 3 else 'Unknown'\n",
    "    else:\n",
    "        frequency = 'Unknown'\n",
    "    \n",
    "    positions = run[\"positions_report\"]\n",
    "    \n",
    "    # Build cumulative P&L from positions\n",
    "    cumulative_pnl = build_cumulative_pnl_from_positions(positions)\n",
    "    \n",
    "    # Create a descriptive label for this run\n",
    "    max_bar_pos = run.get('max_position_bars', 'N/A')\n",
    "    label = f\"{frequency} ({max_bar_pos})\"\n",
    "    \n",
    "    # Add the P&L curve using cumulative P&L index (timestamps)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=cumulative_pnl.index, \n",
    "        y=cumulative_pnl.values, \n",
    "        mode='lines', \n",
    "        name=label,\n",
    "        hovertemplate='<b>' + label + '</b><br>Date: %{x}<br>P&L: $%{y:.2f}<extra></extra>'\n",
    "    ))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    width=1200,\n",
    "    title=f'P&L Curves Comparison for {stock_symbol}',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Realized P&L ($)',\n",
    "    hovermode='x unified',\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.99,\n",
    "        xanchor=\"left\",\n",
    "        x=0.01\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 6. Summary Statistics\n",
    "\n",
    "Display final P&L values for each run to quickly identify top performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display final P&L for each run\n",
    "summary_stats = []\n",
    "\n",
    "for i, run in enumerate(runs_data):\n",
    "    positions = run['positions_report']\n",
    "    \n",
    "    # Extract frequency\n",
    "    bar_type = run.get('bar_type', '')\n",
    "    if bar_type:\n",
    "        parts = bar_type.split('-')\n",
    "        frequency = f\"{parts[1]}-{parts[2]}\" if len(parts) >= 3 else 'Unknown'\n",
    "    else:\n",
    "        frequency = 'Unknown'\n",
    "    \n",
    "    # Calculate final P&L\n",
    "    cumulative_pnl = build_cumulative_pnl_from_positions(positions)\n",
    "    final_pnl = cumulative_pnl.iloc[-1] if len(cumulative_pnl) > 0 else 0\n",
    "    \n",
    "    # Calculate max drawdown\n",
    "    running_max = cumulative_pnl.expanding().max()\n",
    "    drawdown = cumulative_pnl - running_max\n",
    "    max_drawdown = drawdown.min()\n",
    "    \n",
    "    summary_stats.append({\n",
    "        'Run': i + 1,\n",
    "        'Frequency': frequency,\n",
    "        'Max Pos Bars': run.get('max_position_bars', 'N/A'),\n",
    "        'Final P&L ($)': f\"{final_pnl:.2f}\",\n",
    "        'Max Drawdown ($)': f\"{max_drawdown:.2f}\",\n",
    "        'Total Trades': len(positions)\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(summary_stats)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"P&L SUMMARY STATISTICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "print(df_stats.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 7. Detailed Position Comparison\n",
    "\n",
    "Compare positions side-by-side to identify timing and P&L differences between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare positions between the two loaded runs\n",
    "if len(runs_data) == 2:\n",
    "    run1 = runs_data[0]\n",
    "    run2 = runs_data[1]\n",
    "    \n",
    "    # Get positions reports\n",
    "    pos1 = run1['positions_report'].copy()\n",
    "    pos2 = run2['positions_report'].copy()\n",
    "    \n",
    "    # Extract frequency labels\n",
    "    bar_type1 = run1.get('bar_type', '')\n",
    "    bar_type2 = run2.get('bar_type', '')\n",
    "    parts1 = bar_type1.split('-')\n",
    "    parts2 = bar_type2.split('-')\n",
    "    freq1 = f\"{parts1[1]}{parts1[2].lower()}\" if len(parts1) >= 3 else 'Run1'\n",
    "    freq2 = f\"{parts2[1]}{parts2[2].lower()}\" if len(parts2) >= 3 else 'Run2'\n",
    "    \n",
    "    # Convert timestamps and clean P&L\n",
    "    for pos, freq in [(pos1, freq1), (pos2, freq2)]:\n",
    "        # Convert timestamps to datetime\n",
    "        pos['entry_time'] = pd.to_datetime(pos['ts_opened'], unit='ns').apply(\n",
    "            lambda x: convert_utc_to_ny(x.timestamp())\n",
    "        )\n",
    "        pos['exit_time'] = pd.to_datetime(pos['ts_closed'], unit='ns').apply(\n",
    "            lambda x: convert_utc_to_ny(x.timestamp())\n",
    "        )\n",
    "        \n",
    "        # Clean realized P&L\n",
    "        if pos['realized_pnl'].dtype == 'object':\n",
    "            pos['pnl'] = pos['realized_pnl'].str.replace(' USD', '').astype(float)\n",
    "        else:\n",
    "            pos['pnl'] = pos['realized_pnl']\n",
    "        \n",
    "        # Calculate duration in minutes\n",
    "        pos['duration_min'] = (pos['exit_time'] - pos['entry_time']).dt.total_seconds() / 60\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"POSITION-BY-POSITION COMPARISON: {freq1} vs {freq2}\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    print(f\"Total positions - {freq1}: {len(pos1)}, {freq2}: {len(pos2)}\")\n",
    "    print(f\"Total P&L - {freq1}: ${pos1['pnl'].sum():.2f}, {freq2}: ${pos2['pnl'].sum():.2f}\")\n",
    "    print(f\"Difference: ${abs(pos1['pnl'].sum() - pos2['pnl'].sum()):.2f}\\n\")\n",
    "    \n",
    "    # Select key columns for comparison\n",
    "    cols_to_show = ['entry_time', 'exit_time', 'duration_min', 'entry', 'avg_px_open', \n",
    "                    'avg_px_close', 'pnl', 'quantity']\n",
    "    \n",
    "    # Rename for clarity\n",
    "    pos1_display = pos1[cols_to_show].copy()\n",
    "    pos2_display = pos2[cols_to_show].copy()\n",
    "    \n",
    "    pos1_display['run'] = freq1\n",
    "    pos2_display['run'] = freq2\n",
    "    \n",
    "    # Combine and sort by entry time\n",
    "    combined = pd.concat([pos1_display, pos2_display])\n",
    "    combined = combined.sort_values('entry_time').reset_index(drop=True)\n",
    "    \n",
    "    # Display first 20 positions\n",
    "    print(\"First 20 positions (interleaved by entry time):\")\n",
    "    print(combined.head(20).to_string(index=False))\n",
    "    \n",
    "    # Calculate position-level statistics\n",
    "    stats_comparison = pd.DataFrame({\n",
    "        'Metric': ['Avg P&L per trade', 'Win rate', 'Avg duration (min)', \n",
    "                   'Avg winning trade', 'Avg losing trade', 'Largest win', 'Largest loss'],\n",
    "        freq1: [\n",
    "            f\"${pos1['pnl'].mean():.2f}\",\n",
    "            f\"{(pos1['pnl'] > 0).sum() / len(pos1) * 100:.1f}%\",\n",
    "            f\"{pos1['duration_min'].mean():.1f}\",\n",
    "            f\"${pos1[pos1['pnl'] > 0]['pnl'].mean():.2f}\",\n",
    "            f\"${pos1[pos1['pnl'] < 0]['pnl'].mean():.2f}\",\n",
    "            f\"${pos1['pnl'].max():.2f}\",\n",
    "            f\"${pos1['pnl'].min():.2f}\"\n",
    "        ],\n",
    "        freq2: [\n",
    "            f\"${pos2['pnl'].mean():.2f}\",\n",
    "            f\"{(pos2['pnl'] > 0).sum() / len(pos2) * 100:.1f}%\",\n",
    "            f\"{pos2['duration_min'].mean():.1f}\",\n",
    "            f\"${pos2[pos2['pnl'] > 0]['pnl'].mean():.2f}\",\n",
    "            f\"${pos2[pos2['pnl'] < 0]['pnl'].mean():.2f}\",\n",
    "            f\"${pos2['pnl'].max():.2f}\",\n",
    "            f\"${pos2['pnl'].min():.2f}\"\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n\\n{'='*80}\")\n",
    "    print(\"STATISTICAL COMPARISON\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    print(stats_comparison.to_string(index=False))\n",
    "else:\n",
    "    print(\"Please load exactly 2 runs for comparison (modify selected_run_numbers in cell above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 7.1 Entry/Exit Time Alignment Analysis\n",
    "\n",
    "Check if positions are entering/exiting at exactly the same times or if there's a systematic offset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze timing alignment between the two runs\n",
    "if len(runs_data) == 2:\n",
    "    # Sort both by entry time\n",
    "    pos1_sorted = pos1.sort_values('entry_time').reset_index(drop=True)\n",
    "    pos2_sorted = pos2.sort_values('entry_time').reset_index(drop=True)\n",
    "    \n",
    "    # Try to match positions by proximity (within 5 minutes)\n",
    "    matches = []\n",
    "    unmatched_1 = []\n",
    "    unmatched_2 = set(range(len(pos2_sorted)))\n",
    "    \n",
    "    for i, row1 in pos1_sorted.iterrows():\n",
    "        # Find closest entry in pos2 within 5 minutes\n",
    "        time_diffs = abs((pos2_sorted['entry_time'] - row1['entry_time']).dt.total_seconds())\n",
    "        min_diff_idx = time_diffs.idxmin()\n",
    "        min_diff_seconds = time_diffs[min_diff_idx]\n",
    "        \n",
    "        if min_diff_seconds <= 300:  # Within 5 minutes\n",
    "            matches.append({\n",
    "                'pos_num': i + 1,\n",
    "                f'{freq1}_entry': row1['entry_time'],\n",
    "                f'{freq2}_entry': pos2_sorted.loc[min_diff_idx, 'entry_time'],\n",
    "                'entry_diff_sec': (pos2_sorted.loc[min_diff_idx, 'entry_time'] - row1['entry_time']).total_seconds(),\n",
    "                f'{freq1}_exit': row1['exit_time'],\n",
    "                f'{freq2}_exit': pos2_sorted.loc[min_diff_idx, 'exit_time'],\n",
    "                'exit_diff_sec': (pos2_sorted.loc[min_diff_idx, 'exit_time'] - row1['exit_time']).total_seconds(),\n",
    "                f'{freq1}_pnl': row1['pnl'],\n",
    "                f'{freq2}_pnl': pos2_sorted.loc[min_diff_idx, 'pnl'],\n",
    "                'pnl_diff': pos2_sorted.loc[min_diff_idx, 'pnl'] - row1['pnl']\n",
    "            })\n",
    "            unmatched_2.discard(min_diff_idx)\n",
    "        else:\n",
    "            unmatched_1.append(i)\n",
    "    \n",
    "    df_matches = pd.DataFrame(matches)\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"TIMING ALIGNMENT ANALYSIS\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    print(f\"Matched positions: {len(matches)}\")\n",
    "    print(f\"Unmatched in {freq1}: {len(unmatched_1)}\")\n",
    "    print(f\"Unmatched in {freq2}: {len(unmatched_2)}\\n\")\n",
    "    \n",
    "    if len(matches) > 0:\n",
    "        print(\"Entry timing differences (seconds):\")\n",
    "        print(f\"  Mean: {df_matches['entry_diff_sec'].mean():.2f}s\")\n",
    "        print(f\"  Median: {df_matches['entry_diff_sec'].median():.2f}s\")\n",
    "        print(f\"  Std: {df_matches['entry_diff_sec'].std():.2f}s\")\n",
    "        print(f\"  Range: [{df_matches['entry_diff_sec'].min():.2f}s, {df_matches['entry_diff_sec'].max():.2f}s]\")\n",
    "        \n",
    "        print(\"\\nExit timing differences (seconds):\")\n",
    "        print(f\"  Mean: {df_matches['exit_diff_sec'].mean():.2f}s\")\n",
    "        print(f\"  Median: {df_matches['exit_diff_sec'].median():.2f}s\")\n",
    "        print(f\"  Std: {df_matches['exit_diff_sec'].std():.2f}s\")\n",
    "        print(f\"  Range: [{df_matches['exit_diff_sec'].min():.2f}s, {df_matches['exit_diff_sec'].max():.2f}s]\")\n",
    "        \n",
    "        print(\"\\nP&L differences (matched positions only):\")\n",
    "        print(f\"  Mean: ${df_matches['pnl_diff'].mean():.2f}\")\n",
    "        print(f\"  Median: ${df_matches['pnl_diff'].median():.2f}\")\n",
    "        print(f\"  Total: ${df_matches['pnl_diff'].sum():.2f}\")\n",
    "        print(f\"  Range: [${df_matches['pnl_diff'].min():.2f}, ${df_matches['pnl_diff'].max():.2f}]\")\n",
    "        \n",
    "        # Show positions with largest timing differences\n",
    "        print(\"\\n\\nPositions with largest entry timing differences:\")\n",
    "        timing_diffs = df_matches.reindex(df_matches['entry_diff_sec'].abs().nlargest(5).index)\n",
    "        print(timing_diffs[[f'{freq1}_entry', f'{freq2}_entry', 'entry_diff_sec', \n",
    "                           f'{freq1}_pnl', f'{freq2}_pnl', 'pnl_diff']].to_string(index=False))\n",
    "        \n",
    "        # Show positions with largest P&L differences\n",
    "        print(\"\\n\\nPositions with largest P&L differences:\")\n",
    "        pnl_diffs = df_matches.reindex(df_matches['pnl_diff'].abs().nlargest(5).index)\n",
    "        print(pnl_diffs[[f'{freq1}_entry', f'{freq2}_entry', 'entry_diff_sec',\n",
    "                        f'{freq1}_pnl', f'{freq2}_pnl', 'pnl_diff']].to_string(index=False))\n",
    "else:\n",
    "    print(\"Please load exactly 2 runs for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### 7.2 Bar Data Comparison\n",
    "\n",
    "Compare the underlying bar data to see if OHLCV values differ between 1-MINUTE and 60-SECOND bars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare bar data between the two runs\n",
    "if len(runs_data) == 2:\n",
    "    df1 = run1['data'].copy()\n",
    "    df2 = run2['data'].copy()\n",
    "    \n",
    "    # Ensure we have datetime columns\n",
    "    if 'date' in df1.columns:\n",
    "        df1 = df1.rename(columns={'date': 'bar_close_local_datetime'})\n",
    "    if 'date' in df2.columns:\n",
    "        df2 = df2.rename(columns={'date': 'bar_close_local_datetime'})\n",
    "    \n",
    "    # Convert to datetime if needed\n",
    "    if df1['bar_close_local_datetime'].dtype == 'object':\n",
    "        df1['bar_close_local_datetime'] = pd.to_datetime(df1['bar_close_local_datetime'])\n",
    "    if df2['bar_close_local_datetime'].dtype == 'object':\n",
    "        df2['bar_close_local_datetime'] = pd.to_datetime(df2['bar_close_local_datetime'])\n",
    "    \n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"BAR DATA COMPARISON\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    print(f\"Total bars - {freq1}: {len(df1)}, {freq2}: {len(df2)}\")\n",
    "    \n",
    "    # First, check exact timestamp alignment (not rounded)\n",
    "    df1['ts_index'] = range(len(df1))\n",
    "    df2['ts_index'] = range(len(df2))\n",
    "    \n",
    "    # Compare first 10 bar timestamps exactly\n",
    "    print(\"\\n\\nFirst 10 bar timestamps (exact comparison):\")\n",
    "    ts_comparison = pd.DataFrame({\n",
    "        f'{freq1}_timestamp': df1['bar_close_local_datetime'].head(10),\n",
    "        f'{freq2}_timestamp': df2['bar_close_local_datetime'].head(10),\n",
    "        'diff_seconds': (df2['bar_close_local_datetime'].head(10).values - df1['bar_close_local_datetime'].head(10).values) / pd.Timedelta(seconds=1)\n",
    "    })\n",
    "    print(ts_comparison.to_string(index=True))\n",
    "    \n",
    "    # Calculate timestamp differences across all bars\n",
    "    if len(df1) == len(df2):\n",
    "        time_diffs = (df2['bar_close_local_datetime'] - df1['bar_close_local_datetime']).dt.total_seconds()\n",
    "        unique_diffs = time_diffs.unique()\n",
    "        \n",
    "        print(f\"\\n\\nTimestamp offset analysis:\")\n",
    "        print(f\"  Unique offsets found: {len(unique_diffs)}\")\n",
    "        if len(unique_diffs) <= 10:\n",
    "            print(f\"  Offset values (seconds): {sorted(unique_diffs)}\")\n",
    "        else:\n",
    "            print(f\"  Offset range: [{time_diffs.min():.2f}s, {time_diffs.max():.2f}s]\")\n",
    "            print(f\"  Mean offset: {time_diffs.mean():.2f}s\")\n",
    "            print(f\"  Median offset: {time_diffs.median():.2f}s\")\n",
    "    \n",
    "    # Try to merge on timestamp (rounded to minute for alignment)\n",
    "    df1_copy = df1.copy()\n",
    "    df2_copy = df2.copy()\n",
    "    \n",
    "    # Round to nearest minute for joining (using 'min' instead of deprecated 'T')\n",
    "    df1_copy['ts_minute'] = df1_copy['bar_close_local_datetime'].dt.floor('min')\n",
    "    df2_copy['ts_minute'] = df2_copy['bar_close_local_datetime'].dt.floor('min')\n",
    "    \n",
    "    # Merge on rounded minute\n",
    "    merged = df1_copy.merge(\n",
    "        df2_copy, \n",
    "        on='ts_minute', \n",
    "        how='outer', \n",
    "        suffixes=(f'_{freq1}', f'_{freq2}'),\n",
    "        indicator=True\n",
    "    )\n",
    "    \n",
    "    # Count alignment\n",
    "    both = (merged['_merge'] == 'both').sum()\n",
    "    only_1 = (merged['_merge'] == 'left_only').sum()\n",
    "    only_2 = (merged['_merge'] == 'right_only').sum()\n",
    "    \n",
    "    print(f\"\\n\\nBar alignment (rounded to nearest minute):\")\n",
    "    print(f\"  Bars in both: {both}\")\n",
    "    print(f\"  Only in {freq1}: {only_1}\")\n",
    "    print(f\"  Only in {freq2}: {only_2}\")\n",
    "    \n",
    "    # For bars that exist in both, compare OHLCV values\n",
    "    if both > 0:\n",
    "        matched = merged[merged['_merge'] == 'both'].copy()\n",
    "        \n",
    "        # Calculate differences in OHLC\n",
    "        for field in ['open', 'high', 'low', 'close']:\n",
    "            if f'{field}_{freq1}' in matched.columns and f'{field}_{freq2}' in matched.columns:\n",
    "                matched[f'{field}_diff'] = matched[f'{field}_{freq2}'] - matched[f'{field}_{freq1}']\n",
    "        \n",
    "        print(f\"\\n\\nPrice differences for matched bars (n={both}):\")\n",
    "        for field in ['open', 'high', 'low', 'close']:\n",
    "            diff_col = f'{field}_diff'\n",
    "            if diff_col in matched.columns:\n",
    "                diffs = matched[diff_col].dropna()\n",
    "                non_zero = (diffs != 0).sum()\n",
    "                if len(diffs) > 0:\n",
    "                    print(f\"\\n  {field.upper()}:\")\n",
    "                    print(f\"    Bars with differences: {non_zero} ({non_zero/len(diffs)*100:.1f}%)\")\n",
    "                    if non_zero > 0:\n",
    "                        print(f\"    Mean diff: ${diffs.mean():.4f}\")\n",
    "                        print(f\"    Max diff: ${diffs.max():.4f}\")\n",
    "                        print(f\"    Min diff: ${diffs.min():.4f}\")\n",
    "        \n",
    "        # Show sample of bars with differences\n",
    "        print(\"\\n\\nSample of bars with price differences:\")\n",
    "        diffs_exist = False\n",
    "        for field in ['open', 'high', 'low', 'close']:\n",
    "            diff_col = f'{field}_diff'\n",
    "            if diff_col in matched.columns:\n",
    "                if (matched[diff_col] != 0).any():\n",
    "                    diffs_exist = True\n",
    "                    break\n",
    "        \n",
    "        if diffs_exist:\n",
    "            # Find bars with any price difference\n",
    "            matched['has_diff'] = False\n",
    "            for field in ['open', 'high', 'low', 'close']:\n",
    "                diff_col = f'{field}_diff'\n",
    "                if diff_col in matched.columns:\n",
    "                    matched['has_diff'] |= (matched[diff_col] != 0)\n",
    "            \n",
    "            diff_bars = matched[matched['has_diff']].head(10)\n",
    "            cols_to_show = ['ts_minute']\n",
    "            for field in ['open', 'high', 'low', 'close']:\n",
    "                if f'{field}_{freq1}' in diff_bars.columns:\n",
    "                    cols_to_show.extend([f'{field}_{freq1}', f'{field}_{freq2}', f'{field}_diff'])\n",
    "            \n",
    "            print(diff_bars[cols_to_show].to_string(index=False))\n",
    "        else:\n",
    "            print(\"No price differences found in matched bars!\")\n",
    "            print(\"\\nâš ï¸  CONCLUSION: Bar data is IDENTICAL between runs.\")\n",
    "            print(\"   P&L differences must come from timing offsets in bar timestamps,\")\n",
    "            print(\"   causing signals to trigger at slightly different times.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Please load exactly 2 runs for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 7.3 Root Cause Analysis\n",
    "\n",
    "Since bar OHLCV values are identical, the P&L differences must come from timing offsets. This analysis identifies when and how the timing differences affect trading decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root cause: identify how timestamp offsets affect trading decisions\n",
    "if len(runs_data) == 2:\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"ROOT CAUSE ANALYSIS: HOW TIMING OFFSETS AFFECT TRADES\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Check if timestamps differ and by how much\n",
    "    df1 = run1['data'].copy()\n",
    "    df2 = run2['data'].copy()\n",
    "    \n",
    "    if 'date' in df1.columns:\n",
    "        df1['ts'] = pd.to_datetime(df1['date'])\n",
    "    else:\n",
    "        df1['ts'] = pd.to_datetime(df1['bar_close_local_datetime'])\n",
    "    \n",
    "    if 'date' in df2.columns:\n",
    "        df2['ts'] = pd.to_datetime(df2['date'])\n",
    "    else:\n",
    "        df2['ts'] = pd.to_datetime(df2['bar_close_local_datetime'])\n",
    "    \n",
    "    if len(df1) == len(df2):\n",
    "        # Calculate the systematic offset\n",
    "        offset_seconds = (df2['ts'].iloc[0] - df1['ts'].iloc[0]).total_seconds()\n",
    "        \n",
    "        print(f\"Systematic timestamp offset: {offset_seconds} seconds\")\n",
    "        print(f\"  {freq1} first bar: {df1['ts'].iloc[0]}\")\n",
    "        print(f\"  {freq2} first bar: {df2['ts'].iloc[0]}\")\n",
    "        \n",
    "        if abs(offset_seconds) > 0:\n",
    "            print(f\"\\nðŸ’¡ KEY FINDING:\")\n",
    "            print(f\"   Bar timestamps are offset by {offset_seconds} seconds.\")\n",
    "            print(f\"   This means 1-MINUTE bars close on the minute boundary (e.g., 09:31:00),\")\n",
    "            print(f\"   while 60-SECOND bars close {offset_seconds}s {'after' if offset_seconds > 0 else 'before'} the minute.\")\n",
    "            print(f\"\\n   Strategy signals computed on these bars will trigger at different times,\")\n",
    "            print(f\"   leading to different entry/exit prices and ultimately different P&L.\")\n",
    "            \n",
    "            # Check if matched positions have timing that correlates with this offset\n",
    "            if 'df_matches' in locals() and len(df_matches) > 0:\n",
    "                median_entry_diff = df_matches['entry_diff_sec'].median()\n",
    "                print(f\"\\n   Median position entry difference: {median_entry_diff:.1f} seconds\")\n",
    "                print(f\"   This aligns with the {abs(offset_seconds):.1f}s bar timestamp offset!\")\n",
    "        else:\n",
    "            print(f\"\\nâ“ Timestamps are identical, yet positions differ.\")\n",
    "            print(f\"   Investigating other factors...\")\n",
    "            \n",
    "            # Check for differences in position entry logic\n",
    "            print(f\"\\n   Possible causes:\")\n",
    "            print(f\"   - Floating point precision in signal calculations\")\n",
    "            print(f\"   - Race conditions in bar processing order\")\n",
    "            print(f\"   - Different Python/NautilusTrader versions\")\n",
    "    else:\n",
    "        print(f\"Bar counts differ: {len(df1)} vs {len(df2)}\")\n",
    "        print(f\"This suggests different data ranges or filtering.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Please load exactly 2 runs for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 7.4 Close Price Comparison at Exit Times\n",
    "\n",
    "Check if the bar close prices differ at position exit times - this would directly explain P&L differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_data[0]['data'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare close prices at position exit times\n",
    "if len(runs_data) == 2 and 'df_matches' in locals() and len(df_matches) > 0:\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"CLOSE PRICE COMPARISON AT EXIT TIMES\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Get the bar data\n",
    "    df1 = run1['data'].copy()\n",
    "    df2 = run2['data'].copy()\n",
    "    \n",
    "    # Ensure datetime columns\n",
    "    if 'date' in df1.columns:\n",
    "        df1['bar_close_local_datetime'] = pd.to_datetime(df1['date'])\n",
    "    else:\n",
    "        df1['bar_close_local_datetime'] = pd.to_datetime(df1['bar_close_local_datetime'])\n",
    "    \n",
    "    if 'date' in df2.columns:\n",
    "        df2['bar_close_local_datetime'] = pd.to_datetime(df2['date'])\n",
    "    else:\n",
    "        df2['bar_close_local_datetime'] = pd.to_datetime(df2['bar_close_local_datetime'])\n",
    "    \n",
    "    # Determine the close price column name\n",
    "    close_col1 = 'close_price' if 'close_price' in df1.columns else 'close'\n",
    "    close_col2 = 'close_price' if 'close_price' in df2.columns else 'close'\n",
    "    \n",
    "    # Set index for fast lookup\n",
    "    df1_indexed = df1.set_index('bar_close_local_datetime')\n",
    "    df2_indexed = df2.set_index('bar_close_local_datetime')\n",
    "    \n",
    "    # Localize bar timestamps to NY timezone if they're naive (to match position timestamps)\n",
    "    if df1_indexed.index.tz is None:\n",
    "        df1_indexed.index = df1_indexed.index.tz_localize('America/New_York')\n",
    "    if df2_indexed.index.tz is None:\n",
    "        df2_indexed.index = df2_indexed.index.tz_localize('America/New_York')\n",
    "    \n",
    "    # For each matched position, find the close prices at entry and exit\n",
    "    price_comparisons = []\n",
    "    \n",
    "    for idx, row in df_matches.head(20).iterrows():  # Check first 20 matched positions\n",
    "        entry1 = row[f'{freq1}_entry']\n",
    "        entry2 = row[f'{freq2}_entry']\n",
    "        exit1 = row[f'{freq1}_exit']\n",
    "        exit2 = row[f'{freq2}_exit']\n",
    "        \n",
    "        # Find close prices at exit times (or nearest bar)\n",
    "        try:\n",
    "            # Get close price at or before exit time (forward fill)\n",
    "            close1_exit = df1_indexed[close_col1].asof(exit1)\n",
    "            close2_exit = df2_indexed[close_col2].asof(exit2)\n",
    "            \n",
    "            # Get close price at or before entry time\n",
    "            close1_entry = df1_indexed[close_col1].asof(entry1)\n",
    "            close2_entry = df2_indexed[close_col2].asof(entry2)\n",
    "            \n",
    "            price_comparisons.append({\n",
    "                'pos_num': row['pos_num'],\n",
    "                f'{freq1}_exit_time': exit1,\n",
    "                f'{freq2}_exit_time': exit2,\n",
    "                'exit_time_diff_sec': row['exit_diff_sec'],\n",
    "                f'{freq1}_close_at_exit': close1_exit,\n",
    "                f'{freq2}_close_at_exit': close2_exit,\n",
    "                'close_price_diff': close2_exit - close1_exit,\n",
    "                f'{freq1}_pnl': row[f'{freq1}_pnl'],\n",
    "                f'{freq2}_pnl': row[f'{freq2}_pnl'],\n",
    "                'pnl_diff': row['pnl_diff']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not find close prices for position {row['pos_num']}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    df_price_comp = pd.DataFrame(price_comparisons)\n",
    "    \n",
    "    if len(df_price_comp) > 0:\n",
    "        print(f\"Analyzed {len(df_price_comp)} matched positions\\n\")\n",
    "        \n",
    "        # Count positions with close price differences\n",
    "        close_price_diffs = df_price_comp['close_price_diff'].dropna()\n",
    "        non_zero_diffs = (close_price_diffs != 0).sum()\n",
    "        \n",
    "        print(f\"Close price differences at exit times:\")\n",
    "        print(f\"  Positions with different close prices: {non_zero_diffs} ({non_zero_diffs/len(close_price_diffs)*100:.1f}%)\")\n",
    "        \n",
    "        if non_zero_diffs > 0:\n",
    "            print(f\"  Mean price difference: ${close_price_diffs.mean():.4f}\")\n",
    "            print(f\"  Max price difference: ${close_price_diffs.max():.4f}\")\n",
    "            print(f\"  Min price difference: ${close_price_diffs.min():.4f}\")\n",
    "            print(f\"  Std price difference: ${close_price_diffs.std():.4f}\")\n",
    "            \n",
    "            print(\"\\n\\nðŸ’¡ KEY FINDING:\")\n",
    "            print(f\"   Close prices differ at exit times for {non_zero_diffs} positions!\")\n",
    "            print(f\"   Even though OHLCV data looks identical when aligned by minute,\")\n",
    "            print(f\"   the timing offset means positions exit on different bars\")\n",
    "            print(f\"   with different close prices, causing P&L to diverge.\")\n",
    "            \n",
    "            # Show positions with largest close price differences\n",
    "            print(\"\\n\\nPositions with largest close price differences at exit:\")\n",
    "            largest_price_diffs = df_price_comp.reindex(df_price_comp['close_price_diff'].abs().nlargest(5).index)\n",
    "            display_cols = [f'{freq1}_exit_time', f'{freq2}_exit_time', 'exit_time_diff_sec',\n",
    "                          f'{freq1}_close_at_exit', f'{freq2}_close_at_exit', 'close_price_diff',\n",
    "                          f'{freq1}_pnl', f'{freq2}_pnl', 'pnl_diff']\n",
    "            print(largest_price_diffs[display_cols].to_string(index=False))\n",
    "            \n",
    "            # Correlation analysis\n",
    "            if len(close_price_diffs) > 2:\n",
    "                correlation = df_price_comp[['close_price_diff', 'pnl_diff']].corr().iloc[0, 1]\n",
    "                print(f\"\\n\\nCorrelation between close price diff and P&L diff: {correlation:.3f}\")\n",
    "                if abs(correlation) > 0.5:\n",
    "                    print(f\"   Strong correlation! Close price differences ARE causing P&L differences.\")\n",
    "                else:\n",
    "                    print(f\"   Weak correlation. Other factors may also be contributing.\")\n",
    "        else:\n",
    "            print(\"   All close prices are identical at exit times.\")\n",
    "            print(\"   P&L differences must come from other factors (entry prices, fees, etc.)\")\n",
    "        \n",
    "        # Show sample of all comparisons\n",
    "        print(\"\\n\\nSample of position exit price comparisons (first 10):\")\n",
    "        display_cols = [f'{freq1}_exit_time', f'{freq2}_exit_time', \n",
    "                       f'{freq1}_close_at_exit', f'{freq2}_close_at_exit', 'close_price_diff',\n",
    "                       f'{freq1}_pnl', f'{freq2}_pnl', 'pnl_diff']\n",
    "        print(df_price_comp[display_cols].head(10).to_string(index=False))\n",
    "    else:\n",
    "        print(\"Could not extract close price data for comparison.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Need 2 runs with matched positions to compare close prices.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### 7.5 Entry and Exit Price Comparison\n",
    "\n",
    "Compare actual entry (avg_px_open) and exit (avg_px_close) prices from position reports to identify where P&L differences originate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare entry and exit prices from position reports\n",
    "if len(runs_data) == 2 and 'df_matches' in locals() and len(df_matches) > 0:\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(\"ENTRY AND EXIT PRICE COMPARISON (FROM POSITION REPORTS)\")\n",
    "    print(f\"{'='*100}\\n\")\n",
    "    \n",
    "    # Build detailed comparison using position data\n",
    "    pos1 = runs_data[0]['positions_report'].copy()\n",
    "    pos2 = runs_data[1]['positions_report'].copy()\n",
    "    \n",
    "    # Convert timestamps for matching\n",
    "    for pos in [pos1, pos2]:\n",
    "        pos['entry_time'] = pd.to_datetime(pos['ts_opened'], unit='ns').apply(\n",
    "            lambda x: convert_utc_to_ny(x.timestamp())\n",
    "        )\n",
    "        pos['exit_time'] = pd.to_datetime(pos['ts_closed'], unit='ns').apply(\n",
    "            lambda x: convert_utc_to_ny(x.timestamp())\n",
    "        )\n",
    "    \n",
    "    # Sort by entry time\n",
    "    pos1_sorted = pos1.sort_values('entry_time').reset_index(drop=True)\n",
    "    pos2_sorted = pos2.sort_values('entry_time').reset_index(drop=True)\n",
    "    \n",
    "    # Match positions (same logic as before)\n",
    "    price_matches = []\n",
    "    \n",
    "    for i, row1 in pos1_sorted.iterrows():\n",
    "        # Find closest entry in pos2 within 5 minutes\n",
    "        time_diffs = abs((pos2_sorted['entry_time'] - row1['entry_time']).dt.total_seconds())\n",
    "        min_diff_idx = time_diffs.idxmin()\n",
    "        min_diff_seconds = time_diffs[min_diff_idx]\n",
    "        \n",
    "        if min_diff_seconds <= 300:  # Within 5 minutes\n",
    "            row2 = pos2_sorted.loc[min_diff_idx]\n",
    "            \n",
    "            # Extract P&L values\n",
    "            if isinstance(row1['realized_pnl'], str):\n",
    "                pnl1 = float(row1['realized_pnl'].replace(' USD', ''))\n",
    "            else:\n",
    "                pnl1 = float(row1['realized_pnl'])\n",
    "            \n",
    "            if isinstance(row2['realized_pnl'], str):\n",
    "                pnl2 = float(row2['realized_pnl'].replace(' USD', ''))\n",
    "            else:\n",
    "                pnl2 = float(row2['realized_pnl'])\n",
    "            \n",
    "            price_matches.append({\n",
    "                'pos_num': i + 1,\n",
    "                f'{freq1}_entry_time': row1['entry_time'],\n",
    "                f'{freq2}_entry_time': row2['entry_time'],\n",
    "                'entry_time_diff_sec': (row2['entry_time'] - row1['entry_time']).total_seconds(),\n",
    "                f'{freq1}_avg_px_open': float(row1['avg_px_open']),\n",
    "                f'{freq2}_avg_px_open': float(row2['avg_px_open']),\n",
    "                'entry_price_diff': float(row2['avg_px_open']) - float(row1['avg_px_open']),\n",
    "                f'{freq1}_avg_px_close': float(row1['avg_px_close']),\n",
    "                f'{freq2}_avg_px_close': float(row2['avg_px_close']),\n",
    "                'exit_price_diff': float(row2['avg_px_close']) - float(row1['avg_px_close']),\n",
    "                'quantity': int(row1['quantity']),\n",
    "                f'{freq1}_pnl': pnl1,\n",
    "                f'{freq2}_pnl': pnl2,\n",
    "                'pnl_diff': pnl2 - pnl1\n",
    "            })\n",
    "    \n",
    "    df_price_match = pd.DataFrame(price_matches)\n",
    "    \n",
    "    if len(df_price_match) > 0:\n",
    "        print(f\"Analyzed {len(df_price_match)} matched positions\\n\")\n",
    "        \n",
    "        # Entry price analysis\n",
    "        entry_price_diffs = df_price_match['entry_price_diff']\n",
    "        non_zero_entry = (entry_price_diffs != 0).sum()\n",
    "        \n",
    "        print(f\"ENTRY PRICE DIFFERENCES (avg_px_open):\")\n",
    "        print(f\"  Positions with different entry prices: {non_zero_entry} ({non_zero_entry/len(entry_price_diffs)*100:.1f}%)\")\n",
    "        \n",
    "        if non_zero_entry > 0:\n",
    "            print(f\"  Mean difference: ${entry_price_diffs.mean():.6f}\")\n",
    "            print(f\"  Median difference: ${entry_price_diffs.median():.6f}\")\n",
    "            print(f\"  Max difference: ${entry_price_diffs.max():.6f}\")\n",
    "            print(f\"  Min difference: ${entry_price_diffs.min():.6f}\")\n",
    "            print(f\"  Std difference: ${entry_price_diffs.std():.6f}\")\n",
    "            \n",
    "            # Calculate impact on P&L (price diff * quantity)\n",
    "            df_price_match['entry_price_impact'] = df_price_match['entry_price_diff'] * df_price_match['quantity']\n",
    "            print(f\"\\n  Average P&L impact from entry price diff: ${df_price_match['entry_price_impact'].mean():.2f}\")\n",
    "            print(f\"  Total P&L impact from entry price diffs: ${df_price_match['entry_price_impact'].sum():.2f}\")\n",
    "        \n",
    "        # Exit price analysis\n",
    "        exit_price_diffs = df_price_match['exit_price_diff']\n",
    "        non_zero_exit = (exit_price_diffs != 0).sum()\n",
    "        \n",
    "        print(f\"\\n\\nEXIT PRICE DIFFERENCES (avg_px_close):\")\n",
    "        print(f\"  Positions with different exit prices: {non_zero_exit} ({non_zero_exit/len(exit_price_diffs)*100:.1f}%)\")\n",
    "        \n",
    "        if non_zero_exit > 0:\n",
    "            print(f\"  Mean difference: ${exit_price_diffs.mean():.6f}\")\n",
    "            print(f\"  Median difference: ${exit_price_diffs.median():.6f}\")\n",
    "            print(f\"  Max difference: ${exit_price_diffs.max():.6f}\")\n",
    "            print(f\"  Min difference: ${exit_price_diffs.min():.6f}\")\n",
    "            print(f\"  Std difference: ${exit_price_diffs.std():.6f}\")\n",
    "            \n",
    "            # Calculate impact on P&L\n",
    "            df_price_match['exit_price_impact'] = df_price_match['exit_price_diff'] * df_price_match['quantity']\n",
    "            print(f\"\\n  Average P&L impact from exit price diff: ${df_price_match['exit_price_impact'].mean():.2f}\")\n",
    "            print(f\"  Total P&L impact from exit price diffs: ${df_price_match['exit_price_impact'].sum():.2f}\")\n",
    "        \n",
    "        # Combined analysis\n",
    "        print(f\"\\n\\n{'='*80}\")\n",
    "        print(\"ðŸ’¡ ROOT CAUSE SUMMARY\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Calculate correlation between price differences and P&L differences\n",
    "        if non_zero_entry > 0 or non_zero_exit > 0:\n",
    "            df_price_match['total_price_impact'] = df_price_match['entry_price_impact'] + df_price_match['exit_price_impact']\n",
    "            \n",
    "            # For LONG positions, P&L = (exit_price - entry_price) * quantity\n",
    "            # So: pnl_diff = (exit_price2 - entry_price2) - (exit_price1 - entry_price1)\n",
    "            #              = (exit_price_diff - entry_price_diff) * quantity\n",
    "            df_price_match['expected_pnl_diff'] = (df_price_match['exit_price_diff'] - df_price_match['entry_price_diff']) * df_price_match['quantity']\n",
    "            \n",
    "            print(f\"Total P&L difference (actual): ${df_price_match['pnl_diff'].sum():.2f}\")\n",
    "            print(f\"Total P&L difference (expected from price diffs): ${df_price_match['expected_pnl_diff'].sum():.2f}\")\n",
    "            print(f\"Difference explained: {abs(df_price_match['expected_pnl_diff'].sum() / df_price_match['pnl_diff'].sum() * 100):.1f}%\")\n",
    "            \n",
    "            # Correlation\n",
    "            if len(df_price_match) > 2:\n",
    "                corr_entry = df_price_match[['entry_price_impact', 'pnl_diff']].corr().iloc[0, 1]\n",
    "                corr_exit = df_price_match[['exit_price_impact', 'pnl_diff']].corr().iloc[0, 1]\n",
    "                corr_total = df_price_match[['expected_pnl_diff', 'pnl_diff']].corr().iloc[0, 1]\n",
    "                \n",
    "                print(f\"\\nCorrelations with P&L difference:\")\n",
    "                print(f\"  Entry price impact: {corr_entry:.3f}\")\n",
    "                print(f\"  Exit price impact: {corr_exit:.3f}\")\n",
    "                print(f\"  Combined price impact: {corr_total:.3f}\")\n",
    "        \n",
    "        # Show examples with largest entry price differences\n",
    "        print(f\"\\n\\nPositions with largest entry price differences:\")\n",
    "        largest_entry_diffs = df_price_match.reindex(df_price_match['entry_price_diff'].abs().nlargest(10).index)\n",
    "        display_cols = [f'{freq1}_entry_time', f'{freq2}_entry_time', 'entry_time_diff_sec',\n",
    "                       f'{freq1}_avg_px_open', f'{freq2}_avg_px_open', 'entry_price_diff',\n",
    "                       'quantity', 'entry_price_impact', f'{freq1}_pnl', f'{freq2}_pnl', 'pnl_diff']\n",
    "        print(largest_entry_diffs[display_cols].to_string(index=False))\n",
    "        \n",
    "        # Show examples with largest exit price differences\n",
    "        if non_zero_exit > 0:\n",
    "            print(f\"\\n\\nPositions with largest exit price differences:\")\n",
    "            largest_exit_diffs = df_price_match.reindex(df_price_match['exit_price_diff'].abs().nlargest(10).index)\n",
    "            display_cols = [f'{freq1}_exit_time', f'{freq2}_exit_time',\n",
    "                           f'{freq1}_avg_px_close', f'{freq2}_avg_px_close', 'exit_price_diff',\n",
    "                           'quantity', 'exit_price_impact', f'{freq1}_pnl', f'{freq2}_pnl', 'pnl_diff']\n",
    "            print(largest_exit_diffs[[col for col in display_cols if col in df_price_match.columns]].to_string(index=False))\n",
    "        \n",
    "    else:\n",
    "        print(\"Could not match positions for price comparison.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Need 2 runs with matched positions to compare prices.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quiescence (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
